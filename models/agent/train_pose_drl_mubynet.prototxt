name: "Pose-DRL-MubyNet"

# ------------------- GENERAL INPUTS - START -------------------------------

# ------------------------- STATE INPUTS + DUMMY - START -------------------

# If stacked views: 2nd dim = 28, else 2nd dim = 16
layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param { shape: {dim: 40 dim: 128 dim: 23 dim: 34} }
}
layer {
  name: "pred"
  type: "Input"
  top: "pred"
  input_param { shape: {dim: 40 dim: 1} }
}
layer {
  name: "canvas"
  type: "Input"
  top: "canvas"
  input_param { shape: {dim: 40 dim: 45} }
}
layer {
  name: "rig"
  type: "Input"
  top: "rig"
  input_param { shape: {dim: 40 dim: 45} }
}
layer {
  name: "aux"
  type: "Input"
  top: "aux"
  input_param { shape: {dim: 40 dim: 4} }
}

# This contains the constant for the tanh for the elevation angle
layer {
  name: "elev_mult"
  type: "Input"
  top: "elev_mult"
  input_param { shape: {dim: 40 dim: 1} }
}
# ------------------------- STATE INPUTS + DUMMY - END -------------------------

# ----------------------- REWARDS - START --------------------------------------
layer {
  name: "reward_mises"
  type: "Input"
  top: "reward_mises"
  input_param { shape: {dim: 40 dim: 2} }
}
layer {
  name: "reward_binary"
  type: "Input"
  top: "reward_binary"
  input_param { shape: {dim: 40 dim: 1} }
}
layer {
  name: "binary_fake_label"
  type: "Input"
  top: "binary_fake_label"
  input_param { shape: {dim: 40 dim: 1} }
}
layer {
  name: "binary_fake_label_neg"
  type: "Input"
  top: "binary_fake_label_neg"
  input_param { shape: {dim: 40 dim: 1} }
}
# ----------------------- REWARDS - END --------------------------------------

# ----------------------- ACTION INPUT - START -------------------------------

# neg_angle_pred is used in the cosine-shaped loss function in the RL learning
layer {
  name: "neg_angle_pred"
  type: "Input"
  top: "neg_angle_pred"
  input_param { shape: {dim: 40 dim: 2} }
}

# m is the precision parameter of a von Mises distribution
layer {
  name: "m"
  type: "Input"
  top: "m"
  input_param { shape: {dim: 40 dim: 2} }
}

# This contains the constant for the tanh for the azimuth angle.
layer {
  type: "DummyData"
  name: "azim_mult"
  top: "azim_mult"
  dummy_data_param {
    shape: { dim: 40 dim: 1 }
    data_filler: { type: "constant" value: 3.141592653589793 }
  }
}
# ----------------------- ACTION INPUT - END ---------------------------------

# ------------------- GENERAL INPUTS - END ---------------------------------

# ------------------- ADDITIONAL CONV LAYERS FOR BLOB - START ---------------

# conv1 output has size "21 x 21 x 8 x batch-size"
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 8
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2 
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 4
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}

# ------------------- ADDITIONAL CONV LAYERS FOR BLOB - END ---------------

# ---------------------- FLAT+CONCAT INPUT - START ------------------------
layer {
  name: "data_flat"
  type: "Flatten"
  top: "data_flat"
  bottom: "conv2"
}
layer {
  name: "data_pred"
  type: "Concat"
  bottom: "data_flat"
  bottom: "pred"
  top: "data_pred"
}
layer {
  name: "data_canvas"
  type: "Concat"
  bottom: "data_flat"
  bottom: "canvas"
  top: "data_canvas"
}
layer {
  name: "data_rig"
  type: "Concat"
  bottom: "data_flat"
  bottom: "rig"
  top: "data_rig"
}
layer {
  name: "data_pred_canvas"
  type: "Concat"
  bottom: "data_pred"
  bottom: "canvas"
  top: "data_pred_canvas"
}
layer {
  name: "data_pred_rig"
  type: "Concat"
  bottom: "data_pred"
  bottom: "rig"
  top: "data_pred_rig"
}
layer {
  name: "data_canvas_rig"
  type: "Concat"
  bottom: "data_canvas"
  bottom: "rig"
  top: "data_canvas_rig"
}
layer {
  name: "data_pred_canvas_rig"
  type: "Concat"
  bottom: "data_pred_canvas"
  bottom: "rig"
  top: "data_pred_canvas_rig"
}

layer {
  name: "fc1_input"
  type: "Concat"
  bottom: "data_canvas_rig"
  bottom: "aux"
  top: "fc1_input"
}
# ------------------- FLAT+CONCAT INPUT - END ---------------------------------

# -------------------------- TRAINABLE LAYERS - START -----------------------

### MISES -- START ####

layer {
  name: "fc1_mises"
  type: "InnerProduct"
  bottom: "fc1_input"
  top: "fc1_mises"
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "tanh1_mises"
  type: "TanH"
  bottom: "fc1_mises"
  top: "fc1_mises"
}

layer {
  name: "fc2_mises"
  type: "InnerProduct"
  bottom: "fc1_mises"
  top: "fc2_mises"
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "tanh2_mises"
  type: "TanH"
  bottom: "fc2_mises"
  top: "fc2_mises"
}

layer {
  name: "fc3_mises"
  type: "InnerProduct"
  bottom: "fc2_mises"
  top: "fc3_mises"
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

### MISES -- END ####

### CONTINUE -- START ####

layer {
  name: "fc1_continue"
  type: "InnerProduct"
  bottom: "fc1_input"
  top: "fc1_continue"
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "tanh1_continue"
  type: "TanH"
  bottom: "fc1_continue"
  top: "fc1_continue"
}

layer {
  name: "fc2_continue"
  type: "InnerProduct"
  bottom: "fc1_continue"
  top: "fc2_continue"
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "tanh2_continue"
  type: "TanH"
  bottom: "fc2_continue"
  top: "fc2_continue"
}

layer {
  name: "fc3_binary"
  type: "InnerProduct"
  bottom: "fc2_continue"
  top: "fc3_binary"
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

# -------------------------- TRAINABLE LAYERS - END -------------------------

# --------------- ACTION PROBABILITY HEADS - START -------------------------

# The following 3 layers: The von Mises head for spherical prediction
layer {
  name: "tanh"
  type: "TanH"
  bottom: "fc3_mises"
  top: "tanh"
}

# Contains the concatenation for the multiplier for the two angels of the sphere
layer {
  name: "angles_mult"
  type: "Concat"
  bottom: "azim_mult"
  bottom: "elev_mult"
  top: "angles_mult"
}

# The output angels: [azimuth,  elevation]
layer {
  name: "angles"
  type: "Eltwise"
  bottom: "tanh"
  bottom: "angles_mult"
  top: "angles"
  eltwise_param: {
    operation: PROD
  }
}

### CONTINUE ####
layer {
  name: "sigmoid_binary"
  type: "Sigmoid"
  bottom: "fc3_binary"
  top: "sigmoid_binary"
}

# --------------- ACTION PROBABILITY HEADS - END --------------------------

# ---------------------- LOSSES - START -----------------------------------

# ---------------------- LOSS FOR VON MISES - START -----------------------
# This should be negative, since we use EltWise SUM in difference
layer {
  name: "angle_diff"
  type: "Eltwise"
  eltwise_param: {
    operation: SUM
  }
  bottom: "neg_angle_pred"
  bottom: "angles"
  top: "angle_diff"
}
layer {
  name: "cos"
  type: "Cos"
  bottom: "angle_diff"
  top: "cos"
}

layer {
  name: "von_mises"
  type: "Eltwise"
  eltwise_param: {
    operation: PROD
  }
  bottom: "m"
  bottom: "cos"
  top: "von_mises"
}

layer {
  name: "rew_prod_mises"
  type: "Eltwise"
  eltwise_param: {
    operation: PROD
  }
  bottom: "reward_mises"
  bottom: "von_mises"
  top: "rew_prod_mises"
}

layer {
  name: "loss_von_mises"
  type: "Reduction"
  reduction_param: {
    operation: SUM
  }
  bottom: "rew_prod_mises"
  top: "loss_von_mises"
  loss_weight: 1
}
# ---------------------- LOSS FOR VON MISES - END -------------------------

# ---------------------- LOSS FOR binary ACTION - START ---------------------

# y^
layer {
  name: "eps_sigmoid_binary"
  bottom: "sigmoid_binary"
  top: "eps_sigmoid_binary"
  type: "Power"
  power_param {
    power: 1
    scale: 1
    shift: 0.0000001
  }
}

# log(y^)
layer {
  name: "log_eps_sig_binary"
  type: "Log"
  bottom: "eps_sigmoid_binary"
  top: "log_eps_sig_binary"
}

# 1 - y^
layer {
  name: "one_neg_eps_sig_binary"
  bottom: "eps_sigmoid_binary"
  top: "one_neg_eps_sig_binary"
  type: "Power"
  power_param {
    power: 1
    scale: -1.0
    shift: 1
  }
}

# log(1-y^)
layer {
  name: "log_one_neg_eps_sig_binary"
  type: "Log"
  bottom: "one_neg_eps_sig_binary"
  top: "log_one_neg_eps_sig_binary"
}

# y * log(y^)
layer {
  name: "logy_y"
  type: "Eltwise"
  eltwise_param: {
    operation: PROD
  }
  bottom: "log_eps_sig_binary"
  bottom: "binary_fake_label"
  top: "logy_y"
}

# (1-y) * log(1-y^)
layer {
  name: "log_one_neg_y_one_neg_y"
  type: "Eltwise"
  eltwise_param: {
    operation: PROD
  }
  bottom: "log_one_neg_eps_sig_binary"
  bottom: "binary_fake_label_neg"
  top: "log_one_neg_y_one_neg_y"
}

# y * log(y^) + (1 - y) * log(1-y^)
layer {
  name: "cross_entropy_binary"
  type: "Eltwise"
  eltwise_param: {
    operation: SUM
  }
  bottom: "logy_y"
  bottom: "log_one_neg_y_one_neg_y"
  top: "cross_entropy_binary"
}

# R(y * log(y^) + (1 - y) * log(1-y^))
layer {
  name: "rew_prod_ce_binary"
  type: "Eltwise"
  eltwise_param: {
    operation: PROD
  }
  bottom: "cross_entropy_binary"
  bottom: "reward_binary"
  top: "rew_prod_ce_binary"
}

# Reduce over binary actions
layer {
  name: "summed_binary"
  type: "Reduction"
  reduction_param: {
    operation: SUM
    axis: 1
  }
  bottom: "rew_prod_ce_binary"
  top: "summed_binary"
}

# Reduce over batches
layer {
  name: "loss_binary"
  type: "Reduction"
  reduction_param: {
    operation: MEAN
  }
  bottom: "summed_binary"
  top: "loss_binary"
  loss_weight: 1
}

# ---------------------- LOSS FOR binary ACTION - END -----------------------

# ---------------------- LOSSES - END -------------------------------------

layer {
  name: "silence_layer"
  type: "Silence"
  bottom: "data_rig"
  bottom: "data_pred_rig"
  bottom: "data_canvas_rig"
  bottom: "data_pred_canvas_rig"
}